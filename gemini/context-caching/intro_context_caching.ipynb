{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Intro to Context Caching with the Gemini API\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fcontext-caching%2Fintro_context_caching.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/context-caching/intro_context_caching.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://goo.gle/4jeQyWo\">\n",
        "      <img width=\"32px\" src=\"https://cdn.qwiklabs.com/assets/gcp_cloud-e3a77215f0b8bfa9b3f611c0d2208c7e8708ed31.svg\" alt=\"Google Cloud logo\"><br> Open in  Cloud Skills Boost\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Authors |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |\n",
        "| [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The Gemini API provides the context caching feature to store frequently used input tokens in a dedicated cache and use them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model. This feature can help reduce the number of tokens sent to the model, thereby lowering the cost of requests.\n",
        "\n",
        "The Gemini API offers two different caching mechanisms:\n",
        "\n",
        "- Implicit caching (automatic, no cost saving guarantee)\n",
        "- Explicit caching (manual, cost saving guarantee)\n",
        "\n",
        "For more information, refer to the [context caching overview](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) page.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "In this tutorial, you learn how to use implicit caching and explicit caching with the Google Gen AI SDK in Vertex AI.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Use implicit caching\n",
        "- Use explicit caching\n",
        "  - Create a context cache\n",
        "  - Retrieve and use a context cache\n",
        "  - Use context caching in Chat\n",
        "  - Update the expire time of a context cache\n",
        "  - Delete a context cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Google Gen AI SDK for Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87de3b30-49c9-4d80-a697-6a456718a37b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.6/245.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and create client\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"classyellas\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Code Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHwJCyNF6u0O"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mginH0QC6u0O"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    Content,\n",
        "    CreateCachedContentConfig,\n",
        "    GenerateContentConfig,\n",
        "    Part,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6f6d2cbf665"
      },
      "source": [
        "### Create a client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3870ef96f984"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27e7a9b7ac0d"
      },
      "source": [
        "### Use a supported model\n",
        "\n",
        "See context caching [supported models](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview#supported_models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bb92e043e0b3"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\"  # @param [\"gemini-2.0-flash-001\", \"gemini-2.5-flash\", \"gemini-2.5-pro\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "548cbdd322d5"
      },
      "source": [
        "## Implicit caching\n",
        "\n",
        "Implicit caching directly passes cache cost savings to developers without the need to create an explicit cache. Now, when you send a request to one of the Gemini 2.5 models, if the request shares a common prefix as one of previous requests, then it's eligible for a cache hit.\n",
        "\n",
        "**Note** that implicit caching is enabled by default for all Gemini 2.0 and 2.5 models but cost savings only apply to Gemini 2.5 models. The minimum input token count for implicit caching is 2,048 for 2.5 Flash and 2.5 Pro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f2e8b5f2e1c"
      },
      "source": [
        "### Re-enable caching\n",
        "\n",
        "By default, Google foundation models cache inputs for Gemini models. If you disabled caching and want to re-enable it, run the following curl command. To run this command, a user must be granted the Vertex AI administrator role `roles/aiplatform.admin`.\n",
        "\n",
        "For more information about enabling and disabling data caching, see [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/data-governance#enabling-disabling-caching)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f0d7ce598e4d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"API_ENDPOINT\"] = (\n",
        "    f\"{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}\"\n",
        ")\n",
        "os.environ[\"PROJECT_ID\"] = PROJECT_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "25af7a380ff3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd469cc7-7c4e-4852-8400-2949f87aafcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"projects/976996825151/locations/us-central1/projects/976996825151/cacheConfig/operations/7916962437042012160\",\n",
            "  \"done\": true,\n",
            "  \"response\": {\n",
            "    \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.CacheConfig\",\n",
            "    \"name\": \"projects/976996825151/cacheConfig\"\n",
            "  }\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   366    0   285  100    81    729    207 --:--:-- --:--:-- --:--:--   938\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# Enable caching\n",
        "curl -X PATCH \\\n",
        "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  https://${API_ENDPOINT}/cacheConfig \\\n",
        "  -d '{\n",
        "    \"name\": \"projects/${PROJECT_ID}/cacheConfig\",\n",
        "    \"disableCache\": false\n",
        "  }'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a09770b117e7"
      },
      "source": [
        "### Send a request with a large and common content\n",
        "\n",
        "To increase the chance of an implicit cache hit:\n",
        "\n",
        "- Put large and common contents at the beginning of your prompt\n",
        "- Send requests with similar prefix in a short amount of time\n",
        "\n",
        "In this example, you send a request with an image at the beginning of your prompt and add a user's request/question at the end of the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4939e2bc9bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "b2b61dc1-624d-4dcb-e7fe-a0c2d19ecf3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This is a warm and candid selfie-style photograph capturing a man and his dog indoors, likely in a living room.\n\nIn the foreground, a man with dark, slightly curly hair, a beard, and a mustache is smiling broadly directly at the camera. He has warm brown eyes and a friendly expression. He is wearing a cozy-looking, chunky knit blue sweater. His right arm is visible in the lower right corner, suggesting he is holding the camera to take the picture.\n\nTo the man's left, and slightly in front of him, is a medium-sized dog with a mix of black, tan, and white fur. The dog's face is primarily tan with darker markings around its eyes and ears, and a white chest. It has expressive brown eyes and one ear is slightly perked up while the other is floppy, giving it an attentive and slightly curious look. The dog is looking directly at the camera with a gentle expression.\n\nThe background reveals a comfortable home setting. To the left of the dog, part of a grey sofa is visible, and further back, a wooden speaker stands on the floor next to a window with blinds. To the right of the man, behind him, is another part of a sofa with yellow and grey cushions. Further into the room, there's a wooden bookshelf filled with various items and a dining table with chairs near a large window, through which green foliage can be seen. The lighting in the room appears soft and natural, possibly coming from the windows.\n\nThe overall impression is one of a happy, intimate moment shared between a man and his beloved canine companion."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\",\n",
        "            mime_type=\"image/png\",\n",
        "        ),\n",
        "        \"Describe this image.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(PROJECT_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0CzQHhZwqol",
        "outputId": "ca5a36b7-a42d-4fc4-9681-a7e0e1ba584e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classyellas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68d0aaeec4aa"
      },
      "source": [
        "### Send requests with similar prefixes\n",
        "\n",
        "To demonstrate the implicit cache hit, repeat the same request multiple times, and print out the `cached_content_token_count` in the usage metadata which indicates how many tokens in the request were cached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7dd92db38ad9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7202b69-5131-4557-89ea-f5a1387620d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1 Attempt\n",
            "Input tokens: 2334\n",
            "Cached tokens: 0\n",
            "Output tokens: 304\n",
            "Total tokens: 3914\n",
            "\n",
            "#2 Attempt\n",
            "Input tokens: 2334\n",
            "Cached tokens: 0\n",
            "Output tokens: 315\n",
            "Total tokens: 4061\n",
            "\n",
            "#3 Attempt\n",
            "Input tokens: 2334\n",
            "Cached tokens: 2004\n",
            "Output tokens: 268\n",
            "Total tokens: 3961\n",
            "\n",
            "[ModalityTokenCount(\n",
            "  modality=<MediaModality.IMAGE: 'IMAGE'>,\n",
            "  token_count=1994\n",
            "), ModalityTokenCount(\n",
            "  modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "  token_count=10\n",
            ")]\n",
            "Cached content found, exiting loop.\n"
          ]
        }
      ],
      "source": [
        "NUM_ATTEMPTS = 5  # @param {type: \"integer\"}\n",
        "\n",
        "for i in range(NUM_ATTEMPTS):\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=[\n",
        "            Part.from_uri(\n",
        "                file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\",\n",
        "                mime_type=\"image/png\",\n",
        "            ),\n",
        "            \"Write a short and engaging blog post based on this image.\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    cached_token_count = response.usage_metadata.cached_content_token_count or 0\n",
        "\n",
        "    print(f\"#{i + 1} Attempt\")\n",
        "    print(f\"Input tokens: {response.usage_metadata.prompt_token_count}\")\n",
        "    print(f\"Cached tokens: {cached_token_count}\")\n",
        "    print(f\"Output tokens: {response.usage_metadata.candidates_token_count}\")\n",
        "    print(f\"Total tokens: {response.usage_metadata.total_token_count}\")\n",
        "    print()\n",
        "\n",
        "    if cached_token_count > 0:\n",
        "        print(response.usage_metadata.cache_tokens_details)\n",
        "        print(\"Cached content found, exiting loop.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baf87e6e3b64"
      },
      "source": [
        "## Explicit caching\n",
        "\n",
        "Using the explicit caching feature, you can pass some content to the model once, cache the input tokens, and then refer to the cached tokens for subsequent requests. The minimum input token count for context caching is 1,024 for 2.5 Flash and 2,048 for 2.5 Pro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAGPrpYagu2z"
      },
      "source": [
        "### Create a context cache\n",
        "\n",
        "Create a `CachedContent` object specifying the prompt you want to use, including the file and other fields you wish to cache.\n",
        "\n",
        "**Notes**\n",
        "- Caches are model specific. You cannot use a cache made with a different model as their tokenization might be slightly different.\n",
        "- The default expiration time of a context cache is 60 minutes. You can specify a different expiration time using the `ttl` (time to live) or the `expire_time` property.\n",
        "\n",
        "This example shows how to create a context cache using two large research papers stored in a Cloud Storage bucket, and set the `ttl` to 600s.\n",
        "\n",
        "- Paper 1: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)\n",
        "- Paper 2: [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'GoogleCloudOverview.pdf'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'rb') as f:\n",
        "        PDF_BLOB = f.read()\n",
        "    print(f\"Read local file of size: {len(PDF_BLOB)} bytes\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2AKc5EnxYLf",
        "outputId": "0e394c04-6011-4b3e-eeea-98bd944f69b6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read local file of size: 663648 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UmJA6AvVujyZ"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "You are a helpful Google Cloud Assistant, you help the user answer questions related to Google Cloud.\n",
        "You always stick to the facts in the sources provided, and never make up new facts.\n",
        "Now look at the document below, and answer the following questions in 1-2 sentences.\n",
        "\"\"\"\n",
        "\n",
        "cached_content = client.caches.create(\n",
        "    model=MODEL_ID,\n",
        "    config=CreateCachedContentConfig(\n",
        "        contents=[\n",
        "            Content(\n",
        "                role=\"user\",\n",
        "                parts=[\n",
        "                    Part.from_bytes(\n",
        "                        data=PDF_BLOB,\n",
        "                        mime_type=\"application/pdf\",\n",
        "                    ),\n",
        "                ],\n",
        "            )\n",
        "        ],\n",
        "        system_instruction=system_instruction,\n",
        "        ttl=\"600s\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e1dKGSLDg2q"
      },
      "source": [
        "You can access the properties of the cached content as example below. You can use its `name` or `resource_name` to reference the contents of the context cache.\n",
        "\n",
        "**Note**: The `name` of the context cache is also referred to as cache ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uRJPRtkKDk2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2320ff88-fb82-42cf-e25c-2952d4d8a181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "projects/976996825151/locations/us-central1/cachedContents/4915585397347581952\n",
            "projects/classyellas/locations/us-central1/publishers/google/models/gemini-2.5-flash\n",
            "2025-09-18 03:12:26.445465+00:00\n",
            "2025-09-18 03:22:26.366603+00:00\n",
            "audio_duration_seconds=None image_count=7 text_count=224 total_token_count=1868 video_duration_seconds=None\n"
          ]
        }
      ],
      "source": [
        "print(cached_content.name)\n",
        "print(cached_content.model)\n",
        "print(cached_content.create_time)\n",
        "print(cached_content.expire_time)\n",
        "print(cached_content.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-f5gTEaCPkN"
      },
      "source": [
        "### Retrieve a context cache\n",
        "\n",
        "You can use the property `name` to reference the contents of the context cache. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "68e807d418e8"
      },
      "outputs": [],
      "source": [
        "new_cached_content = client.caches.get(name=cached_content.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f0c98e451f8"
      },
      "source": [
        "### Use a context cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ1zMmFQ1BNj"
      },
      "source": [
        "To use the context cache, you provide the `cached_content` resource name in the `config` parameter of the `generate_content()` method.\n",
        "\n",
        "Then you can query the model with a prompt, and the cached content will be used as a prefix to the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EPVyJIW1BaVj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "97622f0c-68a0-4838-a620-dc8a05325f19"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The document provides an overview of Google Cloud, covering its resources, how to access them through services, and how projects are structured. It also describes different ways to interact with Google Cloud services, such as the console, command-line interface, and client libraries."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the topic of the document?\",\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c525ede329b3"
      },
      "source": [
        "You can check cached_content_token_count in the usage metadata which indicates how many tokens in the request were cached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "13afac780407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2eeea46-c515-4fc6-ac3a-5a20b7a40135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens: 1875\n",
            "Cached tokens: 1868\n",
            "Output tokens: 52\n",
            "Total tokens: 1973\n"
          ]
        }
      ],
      "source": [
        "print(f\"Input tokens: {response.usage_metadata.prompt_token_count}\")\n",
        "print(f\"Cached tokens: {response.usage_metadata.cached_content_token_count or 0}\")\n",
        "print(f\"Output tokens: {response.usage_metadata.candidates_token_count}\")\n",
        "print(f\"Total tokens: {response.usage_metadata.total_token_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX7vHiybEWeJ"
      },
      "source": [
        "### Use context caching in chat\n",
        "\n",
        "You can use the context cache in a multi-turn chat session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYNZS5o0FoGR"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U-6wGSFFx51"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "How do the approaches to responsible AI development and mitigation strategies in Gemini 1.5 evolve from those in Gemini 1.0?\n",
        "\"\"\"\n",
        "\n",
        "response = chat.send_message(prompt)\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFO_JgKNeCpK"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Given the advancements presented in Gemini 1.5, what are the key future research directions identified in both papers\n",
        "for further improving multimodal AI models?\n",
        "\"\"\"\n",
        "\n",
        "response = chat.send_message(prompt)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORsGDdXXLwHK"
      },
      "source": [
        "### Update the expiration time of a context cache\n",
        "\n",
        "The default expiration time of a context cache is 60 minutes. To update the expiration time, update one of the following properties:\n",
        "\n",
        "`ttl` - The number of seconds that the cache lives after it's created or after the `ttl` is updated before it expires.\n",
        "\n",
        "`expire_time` - A Timestamp that specifies the absolute date and time when the context cache expires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyiZoZHKI2Jr"
      },
      "outputs": [],
      "source": [
        "cached_content = client.caches.update(\n",
        "    name=cached_content.name,\n",
        "    config=CreateCachedContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        ttl=\"3600s\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(cached_content.expire_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chd6_8YRxdIu"
      },
      "source": [
        "### Delete a context cache\n",
        "\n",
        "You can remove content from the cache using the delete operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGzgTk6YzgSt"
      },
      "outputs": [],
      "source": [
        "client.caches.delete(name=cached_content.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0b4f3814f59"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Learn more about context caching on the [context caching overview](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) page."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_context_caching.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}